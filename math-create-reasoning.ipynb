{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f6ef4ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T23:22:25.352660Z",
     "iopub.status.busy": "2025-05-05T23:22:25.352451Z",
     "iopub.status.idle": "2025-05-05T23:23:36.737477Z",
     "shell.execute_reply": "2025-05-05T23:23:36.736711Z"
    },
    "papermill": {
     "duration": 71.393189,
     "end_time": "2025-05-05T23:23:36.738672",
     "exception": false,
     "start_time": "2025-05-05T23:22:25.345483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-05 23:23:29 __init__.py:183] Automatically detected platform cuda.\n",
      "PyTorch Version: 2.5.1+cu124\n",
      "vLLM: 0.7.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\"\n",
    "\n",
    "import re\n",
    "import random\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "import vllm\n",
    "import requests\n",
    "import json\n",
    "import hmac\n",
    "import hashlib\n",
    "import base64\n",
    "import urllib.parse\n",
    "import datetime\n",
    "import time\n",
    "import gc\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch._dynamo\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "print('PyTorch Version:', torch.__version__)\n",
    "print('vLLM:', vllm.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea7fd8c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T23:23:36.750058Z",
     "iopub.status.busy": "2025-05-05T23:23:36.749807Z",
     "iopub.status.idle": "2025-05-05T23:23:37.256083Z",
     "shell.execute_reply": "2025-05-05T23:23:37.255337Z"
    },
    "papermill": {
     "duration": 0.512939,
     "end_time": "2025-05-05T23:23:37.257224",
     "exception": false,
     "start_time": "2025-05-05T23:23:36.744285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded reference data from /kaggle/input/competition-data/train.csv with 10189 problems\n"
     ]
    }
   ],
   "source": [
    "all_predictions_df = None\n",
    "xls_start_time = None\n",
    "correct_count = 0\n",
    "\n",
    "TORCH_LOGS=\"+dynamo\"\n",
    "TORCHDYNAMO_VERBOSE=1\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "correct_count = 0\n",
    "\n",
    "import kaggle_evaluation.aimo_2_inference_server\n",
    "\n",
    "prompt1 = \"\"\"You are a helpful and harmless assistant. You are Qwen developed by Alibaba. \n",
    "You should think step-by-step. Return final answer within \\\\boxed{}, after taking modulo 1000.\"\"\"\n",
    "\n",
    "prompt2 = \"\"\"You are a the most powerful math expert. Please solve the problems with deep resoning. \n",
    "You are careful and always recheck your conduction. You will never give answer directly until you have enough confidence. \n",
    "You should think step-by-step. Return final answer within \\\\boxed{}, after taking modulo 1000.\"\"\"\n",
    "\n",
    "possible_paths = [\n",
    "    '/kaggle/input/competition-data/train.csv',         \n",
    "    # '/root/AIMO2/reference.csv',      \n",
    "]\n",
    "\n",
    "# 查找第一个存在的文件路径\n",
    "reference_path = None\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        reference_path = path\n",
    "        break\n",
    "\n",
    "if reference_path:\n",
    "    try:\n",
    "        df_ref = pd.read_csv(reference_path)\n",
    "        if 'Question' not in df_ref.columns:\n",
    "            raise ValueError(f\"CSV at {reference_path} missing required columns \")\n",
    "        \n",
    "        question_label_map = dict(zip(df_ref['Question'], df_ref['label']))\n",
    "        print(f\"Successfully loaded reference data from {reference_path} with {len(df_ref)} problems\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "        raise\n",
    "else:\n",
    "    raise FileNotFoundError(\"Cannot find reference.csv in any of the expected locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e70ebb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T23:23:37.268119Z",
     "iopub.status.busy": "2025-05-05T23:23:37.267870Z",
     "iopub.status.idle": "2025-05-05T23:23:37.270845Z",
     "shell.execute_reply": "2025-05-05T23:23:37.270256Z"
    },
    "papermill": {
     "duration": 0.009492,
     "end_time": "2025-05-05T23:23:37.271841",
     "exception": false,
     "start_time": "2025-05-05T23:23:37.262349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.getenv('KAGGLE_KERNEL_RUN_TYPE') or os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    llm_model_7B_pth = '/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1'\n",
    "else:\n",
    "    llm_model_7B_pth = '/kaggle/input/deepseek-r1-distill-qwen-7b/transformers/deepseek-ai-deepseek-r1-distill-qwen-7b/1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e636ca1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T23:23:37.282237Z",
     "iopub.status.busy": "2025-05-05T23:23:37.282028Z",
     "iopub.status.idle": "2025-05-05T23:25:33.234513Z",
     "shell.execute_reply": "2025-05-05T23:25:33.233684Z"
    },
    "papermill": {
     "duration": 115.959358,
     "end_time": "2025-05-05T23:25:33.236116",
     "exception": false,
     "start_time": "2025-05-05T23:23:37.276758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-05 23:24:05 config.py:526] This model supports multiple tasks: {'embed', 'generate', 'score', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 05-05 23:24:09 awq_marlin.py:109] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 05-05 23:24:09 config.py:1383] Defaulting to use mp for distributed inference\n",
      "WARNING 05-05 23:24:09 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "INFO 05-05 23:24:09 llm_engine.py:232] Initializing a V0 LLM engine (v0.7.1) with config: model='/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1', speculative_config=None, tokenizer='/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=24576, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=6312, served_model_name=/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[16,8,4,2,1],\"max_capture_size\":16}, use_cached_outputs=False, \n",
      "WARNING 05-05 23:24:10 multiproc_worker_utils.py:298] Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 05-05 23:24:10 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 05-05 23:24:10 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m INFO 05-05 23:24:10 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 05-05 23:24:10 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\n",
      "WARNING 05-05 23:24:11 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m WARNING 05-05 23:24:11 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 05-05 23:24:11 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 05-05 23:24:11 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "INFO 05-05 23:24:11 cuda.py:235] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 05-05 23:24:11 cuda.py:235] Using Flash Attention backend.\n",
      "INFO 05-05 23:24:11 cuda.py:235] Using Flash Attention backend.\n",
      "INFO 05-05 23:24:11 cuda.py:235] Using Flash Attention backend.\n",
      "INFO 05-05 23:24:22 utils.py:938] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 05-05 23:24:22 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "INFO 05-05 23:24:22 utils.py:938] Found nccl from library libnccl.so.2\n",
      "INFO 05-05 23:24:22 utils.py:938] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 05-05 23:24:22 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "INFO 05-05 23:24:22 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 05-05 23:24:22 utils.py:938] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 05-05 23:24:22 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "WARNING 05-05 23:24:23 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m WARNING 05-05 23:24:23 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 05-05 23:24:23 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 05-05 23:24:23 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 05-05 23:24:23 shm_broadcast.py:256] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_5b23c87c'), local_subscribe_port=38597, remote_subscribe_port=None)\n",
      "INFO 05-05 23:24:23 model_runner.py:1111] Starting to load model /kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 05-05 23:24:23 model_runner.py:1111] Starting to load model /kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1...\n",
      "INFO 05-05 23:24:23 model_runner.py:1111] Starting to load model /kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1...\n",
      "INFO 05-05 23:24:23 model_runner.py:1111] Starting to load model /kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec794c4610e64fa086cf1f53a54d794c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 05-05 23:24:58 model_runner.py:1116] Loading model weights took 1.3375 GB\n",
      "INFO 05-05 23:24:58 model_runner.py:1116] Loading model weights took 1.3375 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m INFO 05-05 23:24:58 model_runner.py:1116] Loading model weights took 1.3375 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 05-05 23:24:58 model_runner.py:1116] Loading model weights took 1.3375 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m WARNING 05-05 23:25:20 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m WARNING 05-05 23:25:20 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 05-05 23:25:20 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m WARNING 05-05 23:25:20 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "INFO 05-05 23:25:20 worker.py:266] Memory profiling takes 20.87 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 05-05 23:25:20 worker.py:266] Memory profiling takes 20.87 seconds\r\n",
      "INFO 05-05 23:25:20 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\r\n",
      "WARNING 05-05 23:25:20 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 05-05 23:25:20 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\r\n",
      "INFO 05-05 23:25:20 worker.py:266] model weights take 1.34GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 1.54GiB; the rest of the memory reserved for KV Cache is 18.12GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m WARNING 05-05 23:25:20 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "INFO 05-05 23:25:20 worker.py:266] model weights take 1.34GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 1.54GiB; the rest of the memory reserved for KV Cache is 18.12GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 05-05 23:25:20 worker.py:266] Memory profiling takes 20.88 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 05-05 23:25:20 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 05-05 23:25:20 worker.py:266] model weights take 1.34GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 1.54GiB; the rest of the memory reserved for KV Cache is 18.12GiB.\n",
      "WARNING 05-05 23:25:20 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 05-05 23:25:20 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "INFO 05-05 23:25:20 worker.py:266] Memory profiling takes 21.22 seconds\r\n",
      "INFO 05-05 23:25:20 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\r\n",
      "INFO 05-05 23:25:20 worker.py:266] model weights take 1.34GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 1.54GiB; the rest of the memory reserved for KV Cache is 18.12GiB.\n",
      "INFO 05-05 23:25:20 executor_base.py:108] # CUDA blocks: 84827, # CPU blocks: 18724\n",
      "INFO 05-05 23:25:20 executor_base.py:113] Maximum concurrency for 24576 tokens per request: 55.23x\n",
      "WARNING 05-05 23:25:20 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 05-05 23:25:20 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m WARNING 05-05 23:25:20 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 05-05 23:25:20 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 05-05 23:25:20 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m WARNING 05-05 23:25:20 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 05-05 23:25:20 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 05-05 23:25:20 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "INFO 05-05 23:25:25 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 05-05 23:25:25 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-05 23:25:25 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-05 23:25:25 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 5/5 [00:05<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 05-05 23:25:31 model_runner.py:1563] Graph capturing finished in 5 secs, took 0.05 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 05-05 23:25:31 model_runner.py:1563] Graph capturing finished in 5 secs, took 0.05 GiB\n",
      "INFO 05-05 23:25:31 model_runner.py:1563] Graph capturing finished in 5 secs, took 0.05 GiB\n",
      "INFO 05-05 23:25:31 model_runner.py:1563] Graph capturing finished in 5 secs, took 0.05 GiB\n",
      "INFO 05-05 23:25:31 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 32.16 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed=0)\n",
    "\n",
    "start_time = time.time()\n",
    "cutoff_time = start_time + (4 * 60 + 50) * 60\n",
    "cutoff_times = [int(x) for x in np.linspace(cutoff_time, start_time + 60 * 60, 50 + 1)]\n",
    "\n",
    "MAX_NUM_SEQS = 16\n",
    "MAX_MODEL_LEN = 8192 * 3 // 2\n",
    "\n",
    "llm = LLM(\n",
    "    llm_model_7B_pth,\n",
    "    max_num_seqs=MAX_NUM_SEQS,          ",
    "    max_model_len=MAX_MODEL_LEN * 2,   ",
    "    trust_remote_code=True,            ",
    "    tensor_parallel_size=4,            ",
    "    gpu_memory_utilization=0.95,      ",
    "    seed=random.randint(1, 10000),\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03def9c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T23:25:33.260471Z",
     "iopub.status.busy": "2025-05-05T23:25:33.260200Z",
     "iopub.status.idle": "2025-05-05T23:25:33.267421Z",
     "shell.execute_reply": "2025-05-05T23:25:33.266820Z"
    },
    "papermill": {
     "duration": 0.020322,
     "end_time": "2025-05-05T23:25:33.268458",
     "exception": false,
     "start_time": "2025-05-05T23:25:33.248136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_ref.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "181a32d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T23:25:33.291939Z",
     "iopub.status.busy": "2025-05-05T23:25:33.291707Z",
     "iopub.status.idle": "2025-05-05T23:25:33.294598Z",
     "shell.execute_reply": "2025-05-05T23:25:33.294027Z"
    },
    "papermill": {
     "duration": 0.015662,
     "end_time": "2025-05-05T23:25:33.295607",
     "exception": false,
     "start_time": "2025-05-05T23:25:33.279945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_reasoning_prompt(question: str) -> str:\n",
    "    return f\"\"\"You are a mathematical assistant helping to analyze the structure of math problems. Given a question, briefly describe what it is asking and mention the main mathematical concepts or theorems likely involved.\n",
    "\n",
    "Keep your explanation short and focused less than 100 tokens . Do not solve or classify the problem.\n",
    "\n",
    "---\n",
    "Problem:\n",
    "{question}\n",
    "\n",
    "Brief analysis:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c841b163",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T23:25:33.319016Z",
     "iopub.status.busy": "2025-05-05T23:25:33.318787Z",
     "iopub.status.idle": "2025-05-05T23:25:33.322707Z",
     "shell.execute_reply": "2025-05-05T23:25:33.322090Z"
    },
    "papermill": {
     "duration": 0.016634,
     "end_time": "2025-05-05T23:25:33.323707",
     "exception": false,
     "start_time": "2025-05-05T23:25:33.307073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Using max_tokens: 200\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 200\n",
    "\n",
    "print(f\"***** Using max_tokens: {max_tokens}\")\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=1.0,               # Randomness of the sampling\n",
    "    top_p=0.90,                    # Cumulative probability of the top tokens to consider\n",
    "    min_p=0.05,                    # Minimum probability for a token to be considered\n",
    "    skip_special_tokens=True,      # Whether to skip special tokens in the output\n",
    "    max_tokens=max_tokens,         # Maximum number of tokens to generate\n",
    "    stop=[\"</think>\"],             # List of strings that stop the generation\n",
    "    seed=random.randint(1, 10000),  ",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "421b5f96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T23:25:33.347037Z",
     "iopub.status.busy": "2025-05-05T23:25:33.346804Z",
     "iopub.status.idle": "2025-05-05T23:25:33.349896Z",
     "shell.execute_reply": "2025-05-05T23:25:33.349299Z"
    },
    "papermill": {
     "duration": 0.015782,
     "end_time": "2025-05-05T23:25:33.350903",
     "exception": false,
     "start_time": "2025-05-05T23:25:33.335121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_reasoning(question):\n",
    "    prompt = create_reasoning_prompt(question)\n",
    "    outputs = llm.generate([prompt], sampling_params)\n",
    "    reasoning = outputs[0].outputs[0].text.strip()\n",
    "    return reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d91f59bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T23:25:33.374101Z",
     "iopub.status.busy": "2025-05-05T23:25:33.373872Z",
     "iopub.status.idle": "2025-05-05T23:25:33.376798Z",
     "shell.execute_reply": "2025-05-05T23:25:33.376206Z"
    },
    "papermill": {
     "duration": 0.01557,
     "end_time": "2025-05-05T23:25:33.377788",
     "exception": false,
     "start_time": "2025-05-05T23:25:33.362218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbc05024",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T23:25:33.401082Z",
     "iopub.status.busy": "2025-05-05T23:25:33.400853Z",
     "iopub.status.idle": "2025-05-05T23:25:33.403386Z",
     "shell.execute_reply": "2025-05-05T23:25:33.402794Z"
    },
    "papermill": {
     "duration": 0.015227,
     "end_time": "2025-05-05T23:25:33.404363",
     "exception": false,
     "start_time": "2025-05-05T23:25:33.389136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df2['reasoning'] = df2['Question'].apply(generate_reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "903d7385",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T23:25:33.427510Z",
     "iopub.status.busy": "2025-05-05T23:25:33.427292Z",
     "iopub.status.idle": "2025-05-05T23:25:33.433892Z",
     "shell.execute_reply": "2025-05-05T23:25:33.433290Z"
    },
    "papermill": {
     "duration": 0.019135,
     "end_time": "2025-05-05T23:25:33.434812",
     "exception": false,
     "start_time": "2025-05-05T23:25:33.415677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts = [create_reasoning_prompt(q) for q in df2[\"Question\"].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "466242a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T23:25:33.458289Z",
     "iopub.status.busy": "2025-05-05T23:25:33.458082Z",
     "iopub.status.idle": "2025-05-06T00:00:34.854443Z",
     "shell.execute_reply": "2025-05-06T00:00:34.853698Z"
    },
    "papermill": {
     "duration": 2101.409465,
     "end_time": "2025-05-06T00:00:34.855608",
     "exception": false,
     "start_time": "2025-05-05T23:25:33.446143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 10189/10189 [34:54<00:00,  4.87it/s, est. speed input: 759.03 toks/s, output: 854.52 toks/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(prompts, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cfc4923",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T00:00:35.606869Z",
     "iopub.status.busy": "2025-05-06T00:00:35.606560Z",
     "iopub.status.idle": "2025-05-06T00:00:35.624888Z",
     "shell.execute_reply": "2025-05-06T00:00:35.624193Z"
    },
    "papermill": {
     "duration": 0.392861,
     "end_time": "2025-05-06T00:00:35.625942",
     "exception": false,
     "start_time": "2025-05-06T00:00:35.233081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reasonings = [output.outputs[0].text.strip() for output in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "896dd1a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T00:00:36.410552Z",
     "iopub.status.busy": "2025-05-06T00:00:36.410239Z",
     "iopub.status.idle": "2025-05-06T00:00:36.415998Z",
     "shell.execute_reply": "2025-05-06T00:00:36.415332Z"
    },
    "papermill": {
     "duration": 0.378184,
     "end_time": "2025-05-06T00:00:36.417080",
     "exception": false,
     "start_time": "2025-05-06T00:00:36.038896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2[\"Reasoning\"] = reasonings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73d363e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T00:00:37.197128Z",
     "iopub.status.busy": "2025-05-06T00:00:37.196795Z",
     "iopub.status.idle": "2025-05-06T00:00:37.226251Z",
     "shell.execute_reply": "2025-05-06T00:00:37.225553Z"
    },
    "papermill": {
     "duration": 0.442212,
     "end_time": "2025-05-06T00:00:37.227283",
     "exception": false,
     "start_time": "2025-05-06T00:00:36.785071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2[\"Reasoning\"] = df2[\"Reasoning\"].apply(lambda x : \".\".join(x.split(\".\")[:-1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c2f29e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T00:00:37.963125Z",
     "iopub.status.busy": "2025-05-06T00:00:37.962812Z",
     "iopub.status.idle": "2025-05-06T00:00:37.967595Z",
     "shell.execute_reply": "2025-05-06T00:00:37.966947Z"
    },
    "papermill": {
     "duration": 0.373215,
     "end_time": "2025-05-06T00:00:37.968660",
     "exception": false,
     "start_time": "2025-05-06T00:00:37.595445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The problem involves the construction of sequences with specific properties related to power sums. The key is to understand how to maximize $d$ given that the same condition must hold for all $f$ from 1 to $d$.\\n\\nTo solve this, we can consider the concept of simultaneous equations where each equation corresponds to a different power of $2f-1$. The goal is to ensure that these equations can be satisfied simultaneously by adjusting the values of $x_i$ appropriately.\\n\\nAnother approach is to analyze the structure of the equations and determine the maximum number of such conditions that can be imposed on the sequence without conflicting with each other.\\n\\nTo find the maximum $d$, it's necessary to consider the interplay between the increasing nature of the $x_i$ sequence and the constraints imposed by the power sums equalling 1 for each $f$.\\n\\nLooking at the problem, I notice that the exponents are odd numbers: 1, 3, 5, ..\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.loc[6619]['Reasoning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6e51d1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T00:00:38.745383Z",
     "iopub.status.busy": "2025-05-06T00:00:38.745078Z",
     "iopub.status.idle": "2025-05-06T00:11:05.799500Z",
     "shell.execute_reply": "2025-05-06T00:11:05.798759Z"
    },
    "papermill": {
     "duration": 627.422943,
     "end_time": "2025-05-06T00:11:05.800615",
     "exception": false,
     "start_time": "2025-05-06T00:00:38.377672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 3044/3044 [10:24<00:00,  4.87it/s, est. speed input: 764.42 toks/s, output: 853.18 toks/s]\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('/kaggle/input/competition-data/test.csv')\n",
    "prompts = [create_reasoning_prompt(q) for q in test[\"Question\"].tolist()]\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "reasonings = [output.outputs[0].text.strip() for output in outputs]\n",
    "test[\"Reasoning\"] = reasonings\n",
    "test[\"Reasoning\"] = test[\"Reasoning\"].apply(lambda x : \".\".join(x.split(\".\")[:-1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa37cab2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T00:11:06.797563Z",
     "iopub.status.busy": "2025-05-06T00:11:06.797239Z",
     "iopub.status.idle": "2025-05-06T00:11:07.089768Z",
     "shell.execute_reply": "2025-05-06T00:11:07.088863Z"
    },
    "papermill": {
     "duration": 0.812569,
     "end_time": "2025-05-06T00:11:07.091116",
     "exception": false,
     "start_time": "2025-05-06T00:11:06.278547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.to_csv('train_reasoning.csv' , index = False)\n",
    "test.to_csv('test_reasoning.csv' , index = False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 11376393,
     "sourceId": 86023,
     "sourceType": "competition"
    },
    {
     "datasetId": 7183993,
     "sourceId": 11464316,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 220483900,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 224053,
     "modelInstanceId": 206829,
     "sourceId": 242129,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 271834,
     "modelInstanceId": 250352,
     "sourceId": 292253,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2931.027074,
   "end_time": "2025-05-06T00:11:12.391011",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-05T23:22:21.363937",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "428be13302f44898b4a8252061a898e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fec41c2c031a4213be1a081e7ca42f26",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_da6f89358b5b4687b86efd536c89a858",
       "tabbable": null,
       "tooltip": null,
       "value": 2.0
      }
     },
     "6c587cd0c7164b459ac1bd99b40c1b46": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6f0ea5bab4b441458d7a0717cee806c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a22f3961843b4897b3fcb5fc0ea72662": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f3c160fb9a0d42379d8a97f21b77261b",
       "placeholder": "​",
       "style": "IPY_MODEL_6f0ea5bab4b441458d7a0717cee806c6",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:34&lt;00:00, 14.76s/it]\n"
      }
     },
     "a7e00ac820694727bb01678bd6aa9c60": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d00062c3309845719f4a29792729d83a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "da6f89358b5b4687b86efd536c89a858": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ec794c4610e64fa086cf1f53a54d794c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ee43145b224e4ee4825e443b0f130ab0",
        "IPY_MODEL_428be13302f44898b4a8252061a898e6",
        "IPY_MODEL_a22f3961843b4897b3fcb5fc0ea72662"
       ],
       "layout": "IPY_MODEL_6c587cd0c7164b459ac1bd99b40c1b46",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ee43145b224e4ee4825e443b0f130ab0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d00062c3309845719f4a29792729d83a",
       "placeholder": "​",
       "style": "IPY_MODEL_a7e00ac820694727bb01678bd6aa9c60",
       "tabbable": null,
       "tooltip": null,
       "value": ""
      }
     },
     "f3c160fb9a0d42379d8a97f21b77261b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fec41c2c031a4213be1a081e7ca42f26": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
