# KAChallenges-Series-1
Classifying Math Problems

Hi everyone,

First, I’d like to thank the host for organizing such a great LLM competition.

### Problem Overview

The task is to classify mathematical questions into one of eight predefined categories .

This is a challenging NLP classification task that requires a deep understanding of both mathematical language and structure.

---

### Methodology

#### 1. **Reasoning-Augmented Input Construction**

To enrich the input data, each question was paired with a brief, structured reasoning generated by the `deepseek-r1-distill-qwen-7b-awq-casperhansen/1` model via the `vllm` framework. This reasoning describes the problem’s mathematical nature without solving or labeling it.

Prompt used:

```python
def create_reasoning_prompt(question: str) -> str:
    return f"""You are a mathematical assistant helping to analyze the structure of math problems. Given a question, briefly describe what it is asking and mention the main mathematical concepts or theorems likely involved.

Keep your explanation short and focused less than 100 tokens. Do not solve or classify the problem.

---
Problem:
{question}

Brief analysis:"""
```

The output of this prompt was stored in a new column, `Reasoning`, in both the train and test sets.

#### 2. **Input Formatting**

Each training example was transformed into a single input string by concatenating the original question with its reasoning:

```python
train['temp'] = "[QUESTION] : " + train['Question'] + ". [REASONING] : " + train['Reasoning']
test['temp'] = "[QUESTION] : " + test['Question'] + ". [REASONING] : " + test['Reasoning']
```

This composite input helps the model leverage both the surface structure and the underlying semantics of the question.

#### 3. **Model Training**

I fine-tuned `deepseek-ai/deepseek-math-7b-base` on this reasoning-augmented dataset.

* **Tokenizer**: Two special tokens were added:

  ```json
  "additional_special_tokens": ["[QUESTION] :", "[REASONING] :"]
  ```
* **Training**:

  * 5-fold stratified cross-validation
  * 2 epochs per fold
  * Fine-tuning via PERT (Parameter-Efficient Fine-Tuning)
  * Trained on a single **NVIDIA P100 GPU**

#### 4. **Ensembling**

Final predictions were made by **averaging the logits** from all 5 folds (simple ensemble).


